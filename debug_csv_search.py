"""
CSVæ¤œç´¢ã®ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ
ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ãŒæ­£ã—ãæ§‹ç¯‰ã•ã‚Œã¦ã„ã‚‹ã‹ã€æ¤œç´¢ãŒæ­£ã—ãå‹•ä½œã™ã‚‹ã‹ã‚’ç¢ºèª
"""
import os
from dotenv import load_dotenv
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_chroma import Chroma
from langchain.text_splitter import CharacterTextSplitter
from langchain.schema import Document
import csv

# ç’°å¢ƒå¤‰æ•°ã®èª­ã¿è¾¼ã¿
load_dotenv()

# å®šæ•°
DATA_DIR_PATH = "./data"
CHUNK_SIZE = 500
CHUNK_OVERLAP = 50
RETRIEVER_SEARCH_K = 5

def load_employee_csv():
    """ç¤¾å“¡åç°¿CSVã‚’éƒ¨ç½²ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã¦èª­ã¿è¾¼ã‚€"""
    csv_path = os.path.join(DATA_DIR_PATH, "ç¤¾å“¡ã«ã¤ã„ã¦", "ç¤¾å“¡åç°¿.csv")
    
    if not os.path.exists(csv_path):
        print(f"âŒ CSVãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {csv_path}")
        return []
    
    print(f"âœ… CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿ä¸­: {csv_path}")
    
    with open(csv_path, 'r', encoding='utf-8') as f:
        reader = csv.DictReader(f)
        rows = list(reader)
    
    print(f"âœ… {len(rows)}è¡Œã®ãƒ‡ãƒ¼ã‚¿ã‚’èª­ã¿è¾¼ã¿ã¾ã—ãŸ")
    
    # éƒ¨ç½²ã”ã¨ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–
    dept_groups = {}
    for row in rows:
        dept = row.get('éƒ¨ç½²', 'ä¸æ˜')
        if dept not in dept_groups:
            dept_groups[dept] = []
        dept_groups[dept].append(row)
    
    print(f"âœ… {len(dept_groups)}å€‹ã®éƒ¨ç½²ã«åˆ†é¡ã•ã‚Œã¾ã—ãŸ:")
    for dept, employees in dept_groups.items():
        print(f"   - {dept}: {len(employees)}å")
    
    # å„éƒ¨ç½²ã”ã¨ã«1ã¤ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆ
    docs = []
    for dept, employees in dept_groups.items():
        content_lines = [f"ã€{dept}ã®å¾“æ¥­å“¡ä¸€è¦§ã€‘\n"]
        for emp in employees:
            emp_info = (
                f"ç¤¾å“¡ID: {emp.get('ç¤¾å“¡ID', '')}, "
                f"æ°å: {emp.get('æ°å(ãƒ•ãƒ«ãƒãƒ¼ãƒ )', '')}, "
                f"æ€§åˆ¥: {emp.get('æ€§åˆ¥', '')}, "
                f"å¹´é½¢: {emp.get('å¹´é½¢', '')}æ­³, "
                f"å¾“æ¥­å“¡åŒºåˆ†: {emp.get('å¾“æ¥­å“¡åŒºåˆ†', '')}, "
                f"éƒ¨ç½²: {emp.get('éƒ¨ç½²', '')}, "
                f"å½¹è·: {emp.get('å½¹è·', '')}, "
                f"ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ: {emp.get('ã‚¹ã‚­ãƒ«ã‚»ãƒƒãƒˆ', '')}, "
                f"ä¿æœ‰è³‡æ ¼: {emp.get('ä¿æœ‰è³‡æ ¼', '')}"
            )
            content_lines.append(emp_info)
        
        content = "\n".join(content_lines)
        doc = Document(page_content=content, metadata={"source": csv_path, "department": dept})
        docs.append(doc)
    
    print(f"âœ… {len(docs)}å€‹ã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    # æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆã®å†…å®¹ã‚’ã‚µãƒ³ãƒ—ãƒ«è¡¨ç¤º
    if docs:
        print(f"\nğŸ“„ ã‚µãƒ³ãƒ—ãƒ«(æœ€åˆã®ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ):")
        print(f"éƒ¨ç½²: {docs[0].metadata['department']}")
        print(f"å†…å®¹(æœ€åˆã®500æ–‡å­—):\n{docs[0].page_content[:500]}...\n")
    
    return docs

def create_vector_store(docs):
    """ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆ"""
    print("\nğŸ”§ ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆä¸­...")
    
    # ãƒ†ã‚­ã‚¹ãƒˆåˆ†å‰²
    text_splitter = CharacterTextSplitter(
        separator="\n",
        chunk_size=CHUNK_SIZE,
        chunk_overlap=CHUNK_OVERLAP
    )
    texts = text_splitter.split_documents(docs)
    print(f"âœ… {len(texts)}å€‹ã®ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¾ã—ãŸ")
    
    # Embeddingãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–
    embeddings = OpenAIEmbeddings(model="text-embedding-3-small")
    
    # ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = Chroma.from_documents(
        documents=texts,
        embedding=embeddings,
        persist_directory="./.chroma_debug"
    )
    print("âœ… ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã‚’ä½œæˆã—ã¾ã—ãŸ")
    
    return vectorstore

def test_search(vectorstore, query):
    """æ¤œç´¢ãƒ†ã‚¹ãƒˆ"""
    print(f"\nğŸ” æ¤œç´¢ã‚¯ã‚¨ãƒª: '{query}'")
    
    # Retrieverã®ä½œæˆ
    retriever = vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={"k": RETRIEVER_SEARCH_K}
    )
    
    # æ¤œç´¢å®Ÿè¡Œ
    results = retriever.invoke(query)
    
    print(f"âœ… {len(results)}ä»¶ã®çµæœãŒè¦‹ã¤ã‹ã‚Šã¾ã—ãŸ:\n")
    
    for i, doc in enumerate(results, 1):
        print(f"--- çµæœ {i} ---")
        print(f"éƒ¨ç½²: {doc.metadata.get('department', 'N/A')}")
        print(f"å†…å®¹(æœ€åˆã®300æ–‡å­—):\n{doc.page_content[:300]}...\n")
    
    return results

def main():
    print("=" * 60)
    print("CSVæ¤œç´¢ãƒ‡ãƒãƒƒã‚°ã‚¹ã‚¯ãƒªãƒ—ãƒˆ")
    print("=" * 60)
    
    # 1. CSVãƒ‡ãƒ¼ã‚¿ã®èª­ã¿è¾¼ã¿
    docs = load_employee_csv()
    
    if not docs:
        print("âŒ ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆãŒä½œæˆã§ãã¾ã›ã‚“ã§ã—ãŸ")
        return
    
    # 2. ãƒ™ã‚¯ãƒˆãƒ«ã‚¹ãƒˆã‚¢ã®ä½œæˆ
    vectorstore = create_vector_store(docs)
    
    # 3. æ¤œç´¢ãƒ†ã‚¹ãƒˆ
    test_queries = [
        "äººäº‹éƒ¨ã«æ‰€å±ã—ã¦ã„ã‚‹å¾“æ¥­å“¡",
        "äººäº‹éƒ¨",
        "å–¶æ¥­éƒ¨ã®ç¤¾å“¡",
        "ITéƒ¨ã®ã‚¹ã‚¿ãƒƒãƒ•"
    ]
    
    for query in test_queries:
        test_search(vectorstore, query)
    
    print("\n" + "=" * 60)
    print("ãƒ‡ãƒãƒƒã‚°å®Œäº†")
    print("=" * 60)

if __name__ == "__main__":
    main()
